# LAB2-1 Report

作者：丁思源 2022012901

---

## 简单数据集上的 ID3 和 CART 决策树构建

我们约定每个数据采用“属性二进制（类别）”的形式来表示。例如 $101(2)$

### ID3 算法

根节点的数据集合是 $010(1), 100(2), 011(2), 101(2), 000(2), 011(1)$

根据定义，根节点的 $p_1 = \frac{1}{3}, p_2 = \frac{2}{3}$，则 $\operatorname{Ent}(D) = -\frac{1}{3}\log_2\frac{1}{3} - \frac{2}{3}\log_2\frac{2}{3} \approx 0.9182958340544896$

- 采用第一个属性进行划分成两个子集：
  
  $010(1), 011(2), 000(2), 011(1) \qquad 100(2), 101(2)$

  第一个子集中，$\operatorname{Ent}(D_0) = -\frac{1}{2}\log_2\frac{1}{2} - \frac{1}{2}\log_2\frac{1}{2} = 1$

  第二个子集中，$\operatorname{Ent}(D_1) = 0$

  故这种划分的 $\operatorname{Gain}(D, 1) \approx 0.2516291673878229$
- 采用第二个属性进行划分成两个子集：
  
  $100(2), 101(2), 000(2) \qquad 010(1), 011(2), 011(1)$

  第一个子集中，$\operatorname{Ent}(D_0) = 0$

  第二个子集中，$\operatorname{Ent}(D_1) = -\frac{2}{3}\log_2\frac{2}{3} - \frac{1}{3}\log_2\frac{1}{3} = 0.9182958340544896$

  故这种划分的 $\operatorname{Gain}(D, 2) \approx 0.4591479170272448$
- 采用第三个属性进行划分成两个子集：
  
  $010(1), 100(2), 000(2) \qquad 011(2), 101(2), 011(1)$

  第一个子集中，$\operatorname{Ent}(D_0) = -\frac{1}{3}\log_2\frac{1}{3} - \frac{2}{3}\log_2\frac{2}{3} = 0.9182958340544896$

  第二个子集中，$\operatorname{Ent}(D_1) = -\frac{1}{3}\log_2\frac{1}{3} - \frac{2}{3}\log_2\frac{2}{3} = 0.9182958340544896$

  故这种划分的 $\operatorname{Gain}(D, 3) = 0$

因此我们采用第二个属性进行划分。

获得两个子节点：$100(2), 101(2), 000(2)$ 和 $010(1), 011(2), 011(1)$。

其中左节点已经满足终止条件了，右节点按照第一、二个属性划分得到的信息增益都是 $0$，而按照第三个属性划分，得到 $\operatorname{Gain}(D, 3) \approx 0.2516291673878229$，得到的两个子节点都是终止状态。

综上所述，ID3 算法构建的决策树如下图所示：

![ID3_Tree](https://s1.ax1x.com/2023/04/06/ppoMudJ.png)

### CART 算法

根节点的数据集合是 $010(1), 100(2), 011(2), 101(2), 000(2), 011(1)$

- 采用第一个属性进行划分成两个子集：
  
  $010(1), 011(2), 000(2), 011(1) \qquad 100(2), 101(2)$

  第一个子集中，$G(D_0) = 1 - \left(\frac{1}{2}\right)^2 - \left(\frac{1}{2}\right)^2 = 0.5$

  第二个子集中，$G(D_1) = 0$

  故这种划分的 $\operatorname{Gini}(D, 1) = \frac{1}{3}$
- 采用第二个属性进行划分成两个子集：
  
  $100(2), 101(2), 000(2) \qquad 010(1), 011(2), 011(1)$

  第一个子集中，$G(D_0) = 0$

  第二个子集中，$G(D_1) = 1 - \left(\frac{2}{3}\right)^2 - \left(\frac{1}{3}\right)^2 = \frac{4}{9}$

  故这种划分的 $\operatorname{Gini}(D, 2) = \frac{2}{9}$
- 采用第三个属性进行划分成两个子集：
  
  $010(1), 100(2), 000(2) \qquad 011(2), 101(2), 011(1)$

  第一个子集中，$G(D_0) = 1 - \left(\frac{1}{3}\right)^2 - \left(\frac{2}{3}\right)^2 = \frac{4}{9}$

  第二个子集中，$G(D_1) = 1 - \left(\frac{1}{3}\right)^2 - \left(\frac{2}{3}\right)^2 = \frac{4}{9}$

  故这种划分的 $\operatorname{Gini}(D, 3) = \frac{4}{9}$

因此我们采用第二个属性进行划分。

获得两个子节点：$100(2), 101(2), 000(2)$ 和 $010(1), 011(2), 011(1)$。

其中左节点已经满足终止条件了，右节点按照第一、二个属性划分都无法产生新的区分，而按照第三个属性划分，得到两个子节点，且这两个子节点都是终止状态。

可以发现，在这个数据集上，CART 算法和 ID3 算法得到的决策树是完全相同的。

---

## 决策树实现思路简述

ID3 和 CART 算法的本质是相同的，唯一的区别是每次选择属性时，判断的指标不相同（分别是信息增益和基尼系数），因此两者的实现思路是类似的。

1. 当前节点的数据集是 $S$，可供选择的划分属性集合为 $A$。
2. 如果当前数据集满足终止条件，计算当前节点对应的类别决策结果。退出当前过程。
3. 对于每个 $a \in A$，根据属性 $a$ 将 $S$ 划分成两个子集 $S_{a,0}$ 和 $S_{a,1}$。利用 ID3 或 CART 的判断指标计算每种划分的信息增益或基尼系数。
4. 选择最优的划分属性 $a_0$，此时对应的划分是 $S_0$ 和 $S_1$，新的可供选择的划分集合为 $A' = A / \{a_0\}$。
5. 利用 $S_0, A'$ 和 $S_1, A'$ 递归进行过程 1。

---

## 在测试集上的表现

经过训练和测试，发现 ID3 算法构建的决策树在测试集上的准确率为 $77.5\%$，CART 算法的准确率则为 $78.5\%$，后者略高于前者（不过差异不显著）。

---

## 思考题

1. 多离散值：当某个属性的离散值集合为 $\{S_1, S_2, \dots, S_k\}$ 时，我们可以采用“是否为 $S_1$”，“是否为 $S_2$”，……，“是否为 $S_k$”作为决策树节点的判断条件，从而将多离散值转化为二元值。
2. 连续值：当某个属性的取值 $x$ 是连续值时，我们可以采用“$x$ 是否小于 $X_1$”，“$x$ 是否在 $[X_1, X_2)$ 内”，“$x$ 是否在 $[X_2, X_3)$ 内”……，“$x$ 是否在 $[X_{k - 1}, X_k)$ 内”，“$x$ 是否不小于 $X_k$”作为判断条件，从而将连续值转化为二元值。
3. 多离散值时，二叉和多叉对比：
   - 二叉树的劣势：所需要的比较次数更多，需要的决策树节点个数更多，从而带来更大的判断时间开销。
   - 二叉树的优势：表面上，整棵决策树中每个非叶子节点的子结点个数相同，结构更统一；实际上，对于某个多离散值的属性 $S$，如果在测试集中存在训练集中不存在的离散值时，多叉树无法对这个数据进行类别预测（没有一个分支满足条件），而二叉树仍然可以进行类别预测，泛化能力和鲁棒性更强。
4. 生活中的例子
   - 二值：生物中的二歧分类法、MBTI十六型人格
   - 多离散值：某门课中学生的绩点和平时、期中、期末等各方面成绩或表现的关系
   - 连续值：水的状态和温度、压强的关系
